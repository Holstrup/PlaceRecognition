\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Designing a GPS-based loss function for visual place recognition}

\author{\IEEEauthorblockN{Alexander Holstrup}
\IEEEauthorblockA{DTU Compute \\
\textit{Technical University of Denmark}\\
Copenhagen, Denmark \\
abho@dtu.dk}
}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}

% What is the problem and motivation?
Visual place recognition is the task of matching a query image of a particular location, with other images from the same location. The need for this task to be
solved is motivated by its applications in areas such as autonomous driving, \textbf{(...)}. This task is often solved as an instance of the image retrieval
problem, where each database image is encoded as an n-dimensional vector, and the location of query images is determined by encoding the the query image, and finding
images close to it in the vector space. Traditionally, the images were encoded using invariant features, like SIFT, however the recent advancements in deep learning, 
opened up for Convolutional Neural Nets (CNNs) to be used for this task as well. \\

% What does this paper contribute?
At this point it is natural to question, how we define if two images are in the same location. The majority of existing research consider two images taken in the 
same location, if their GPS coordinates are 25 meters of each other. This definition mainly stems from the fact, that existing datasets have an uncertainly on 
the GPS coordinates of up to 15 meters, which leaves 10 meters as a buffer. This means, that an images further away than 25 meters from a particular query image 
will all simply be considered incorrect, regardless of how far away their true distance was. In this paper, we try to relax this condition to explore if this 
will help improve the performance of existing SOTA frameworks. Specifically, we will present a new loss function for this task, that takes the true distances into 
consideration.

% Rest of the paper is structured as 

\section{Related Work}
\textbf{Existing Frameworks} \\
% NetVLAD

% Fine-tuning CNN Image Retrieval with No Human Annotation

\textbf{Loss Functions} \\
% Triplet loss 

% Contrastive loss 

\section{Methodology}

% TODO: Later

\section{Experiments}

% TODO: Later

\section{Discussion}

% TODO: Later

\section{Conclusion}

% TODO: Later

\section*{Acknowledgment}

% Thank you to Frederik Warbrug & SÃ¸ren Hauberg

\section*{References}

Please number citations consecutively within brackets \cite{IEEEhowto:IEEEtranpage}. The 




\bibliographystyle{./bibliography/IEEEtran}
\bibliography{./bibliography/IEEEexample}

\end{document}
